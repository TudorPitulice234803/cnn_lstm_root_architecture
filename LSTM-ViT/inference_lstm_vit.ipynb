{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39414e9-6be9-449b-bed8-556d65b5718b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 models to evaluate:\n",
      "  - lstm_vit_run3.h5\n",
      "  - lstm_vit_run1.h5\n",
      "  - lstm_vit_run4.h5\n",
      "  - lstm_vit_run5.h5\n",
      "  - lstm_vit_run2.h5\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 14:37:58.922170: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping frame 28_05_01.png as ground truth has no positive pixels.\n",
      "\n",
      "  Results for lstm_vit_run3:\n",
      "    F1 Score:  0.7648\n",
      "    mIoU:      0.6193\n",
      "    Precision: 0.9065\n",
      "    Recall:    0.6621\n",
      "    Frames:    14\n",
      "\n",
      "============================================================\n",
      "\n",
      "  Skipping frame 28_05_01.png as ground truth has no positive pixels.\n",
      "\n",
      "  Results for lstm_vit_run1:\n",
      "    F1 Score:  0.7775\n",
      "    mIoU:      0.6361\n",
      "    Precision: 0.9250\n",
      "    Recall:    0.6715\n",
      "    Frames:    14\n",
      "\n",
      "============================================================\n",
      "\n",
      "  Skipping frame 28_05_01.png as ground truth has no positive pixels.\n",
      "\n",
      "  Results for lstm_vit_run4:\n",
      "    F1 Score:  0.7766\n",
      "    mIoU:      0.6349\n",
      "    Precision: 0.9181\n",
      "    Recall:    0.6738\n",
      "    Frames:    14\n",
      "\n",
      "============================================================\n",
      "\n",
      "  Skipping frame 28_05_01.png as ground truth has no positive pixels.\n",
      "\n",
      "  Results for lstm_vit_run5:\n",
      "    F1 Score:  0.7719\n",
      "    mIoU:      0.6288\n",
      "    Precision: 0.9217\n",
      "    Recall:    0.6644\n",
      "    Frames:    14\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: 5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fed0c519080> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Skipping frame 28_05_01.png as ground truth has no positive pixels.\n",
      "\n",
      "  Results for lstm_vit_run2:\n",
      "    F1 Score:  0.0000\n",
      "    mIoU:      0.0000\n",
      "    Precision: 0.0000\n",
      "    Recall:    0.0000\n",
      "    Frames:    14\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "SUMMARY OF ALL MODELS\n",
      "============================================================\n",
      "   model_name       f1      iou  precision   recall  num_frames       model_file\n",
      "lstm_vit_run1 0.777511 0.636110   0.924989 0.671534          14 lstm_vit_run1.h5\n",
      "lstm_vit_run4 0.776620 0.634950   0.918117 0.673817          14 lstm_vit_run4.h5\n",
      "lstm_vit_run5 0.771882 0.628756   0.921687 0.664412          14 lstm_vit_run5.h5\n",
      "lstm_vit_run3 0.764827 0.619317   0.906478 0.662080          14 lstm_vit_run3.h5\n",
      "lstm_vit_run2 0.000000 0.000000   0.000000 0.000000          14 lstm_vit_run2.h5\n",
      "\n",
      "============================================================\n",
      "  F1 Score:  0.7775\n",
      "  mIoU:      0.6361\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from patchify import patchify, unpatchify\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score as f1_metric\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from preprocess import crop, padder, crop_to_coordinates\n",
    "from helpers import f1\n",
    "\n",
    "def inference_single_model(model, image_dir, time_steps=15, kernel_size=5, patch_size=256):\n",
    "    \"\"\"\n",
    "    Run inference for a single model and return metrics.\n",
    "    \"\"\"\n",
    "    # Load image file names\n",
    "    files = sorted([f for f in os.listdir(image_dir) if f.endswith('.png')])\n",
    "    mask_dir = image_dir\n",
    "\n",
    "    # Metrics storage\n",
    "    all_f1, all_iou, all_precision, all_recall = [], [], [], []\n",
    "\n",
    "    num_sequences = len(files) // time_steps\n",
    "    for seq_idx in range(num_sequences):\n",
    "        seq_files = files[seq_idx * time_steps:(seq_idx + 1) * time_steps]\n",
    "        frames_padded = []\n",
    "        stats, centroids = None, None\n",
    "\n",
    "        # Preprocess each frame in the sequence\n",
    "        for file in seq_files:\n",
    "            img_path = os.path.join(image_dir, file)\n",
    "            image = cv2.imread(img_path, 0)  # Grayscale\n",
    "            image_cropped, stats, centroids = crop(image, kernel_size=kernel_size)\n",
    "            image_color = cv2.cvtColor(image_cropped, cv2.COLOR_GRAY2RGB)\n",
    "            image_np = image_color / 255.0\n",
    "            image_padded = padder(image_np, patch_size=patch_size)\n",
    "            frames_padded.append(image_padded)\n",
    "\n",
    "        frames_padded = np.array(frames_padded)  # (time_steps, H, W, 3)\n",
    "        H, W, _ = frames_padded[0].shape\n",
    "\n",
    "        # Patchify all frames\n",
    "        patches_seq = []\n",
    "        for t in range(time_steps):\n",
    "            patches = patchify(frames_padded[t], (patch_size, patch_size, 1), step=patch_size)\n",
    "            patches_seq.append(patches)\n",
    "\n",
    "        patches_seq = np.stack(patches_seq, axis=2)\n",
    "        num_x, num_y = patches_seq.shape[:2]\n",
    "\n",
    "        # Reshape to (num_patches, time_steps, patch_size, patch_size, 3)\n",
    "        patches_seq = patches_seq.reshape(-1, time_steps, patch_size, patch_size, 1)\n",
    "\n",
    "        # Predict masks for all time steps\n",
    "        predicted_patches = model.predict(patches_seq, verbose=0)\n",
    "\n",
    "        # Reshape back to (num_x, num_y, time_steps, patch_size, patch_size)\n",
    "        predicted_patches = predicted_patches.reshape(num_x, num_y, time_steps, patch_size, patch_size)\n",
    "\n",
    "        # Process each frame\n",
    "        for t in range(time_steps):\n",
    "            predicted_mask = unpatchify(predicted_patches[:, :, t, :, :], (H, W))\n",
    "            prediction_bool = (predicted_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            # Load corresponding ground truth mask\n",
    "            mask_filename = seq_files[t].replace('.png', '.tif')\n",
    "            mask_path = os.path.join(mask_dir, mask_filename)\n",
    "            if not os.path.exists(mask_path):\n",
    "                print(f\"  Mask for {seq_files[t]} not found, skipping frame...\")\n",
    "                continue\n",
    "\n",
    "            mask = cv2.imread(mask_path, 0)\n",
    "            mask = crop_to_coordinates(mask, stats, centroids)\n",
    "            mask = padder(mask, patch_size=patch_size)\n",
    "            mask_bool = (mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            if np.sum(mask_bool) == 0:\n",
    "                print(f\"  Skipping frame {seq_files[t]} as ground truth has no positive pixels.\")\n",
    "                continue\n",
    "\n",
    "            # Compute metrics\n",
    "            pred_flat = prediction_bool.flatten()\n",
    "            mask_flat = mask_bool.flatten()\n",
    "            f1_val = f1_metric(mask_flat, pred_flat)\n",
    "            iou_val = jaccard_score(mask_flat, pred_flat)\n",
    "            precision_val = precision_score(mask_flat, pred_flat, zero_division=0)\n",
    "            recall_val = recall_score(mask_flat, pred_flat, zero_division=0)\n",
    "\n",
    "            all_f1.append(f1_val)\n",
    "            all_iou.append(iou_val)\n",
    "            all_precision.append(precision_val)\n",
    "            all_recall.append(recall_val)\n",
    "\n",
    "    # Calculate average metrics\n",
    "    metrics = {\n",
    "        'f1': np.mean(all_f1) if all_f1 else 0.0,\n",
    "        'iou': np.mean(all_iou) if all_iou else 0.0,\n",
    "        'precision': np.mean(all_precision) if all_precision else 0.0,\n",
    "        'recall': np.mean(all_recall) if all_recall else 0.0,\n",
    "        'num_frames': len(all_f1)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def pipeline_all_models(models_dir, image_dir, time_steps=15, kernel_size=5, patch_size=256, \n",
    "                       model_extensions=['.h5', '.keras'], save_results=True):\n",
    "    \"\"\"\n",
    "    Run inference on all models in a directory and compile results.\n",
    "    \n",
    "    Args:\n",
    "        models_dir: Directory containing all model files\n",
    "        image_dir: Directory containing test images\n",
    "        time_steps: Number of time steps for LSTM models\n",
    "        kernel_size: Kernel size for preprocessing\n",
    "        patch_size: Patch size for patchifying\n",
    "        model_extensions: List of model file extensions to look for\n",
    "        save_results: Whether to save results to CSV\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with results for all models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all model files\n",
    "    model_files = []\n",
    "    for ext in model_extensions:\n",
    "        model_files.extend([f for f in os.listdir(models_dir) if f.endswith(ext)])\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"No model files found in {models_dir} with extensions {model_extensions}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Found {len(model_files)} models to evaluate:\")\n",
    "    for mf in model_files:\n",
    "        print(f\"  - {mf}\")\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Process each model\n",
    "    for idx, model_file in enumerate(model_files, 1):\n",
    "        model_path = os.path.join(models_dir, model_file)\n",
    "        model_name = os.path.splitext(model_file)[0]\n",
    "        \n",
    "        try:\n",
    "            # Load model\n",
    "            model = load_model(model_path, custom_objects={\"f1\": f1})\n",
    "            \n",
    "            # Run inference\n",
    "            metrics = inference_single_model(\n",
    "                model, \n",
    "                image_dir, \n",
    "                time_steps=time_steps,\n",
    "                kernel_size=kernel_size,\n",
    "                patch_size=patch_size\n",
    "            )\n",
    "            \n",
    "            # Add model info to metrics\n",
    "            metrics['model_name'] = model_name\n",
    "            metrics['model_file'] = model_file\n",
    "            \n",
    "            # Print results for this model\n",
    "            print(f\"\\n  Results for {model_name}:\")\n",
    "            print(f\"    F1 Score:  {metrics['f1']:.4f}\")\n",
    "            print(f\"    mIoU:      {metrics['iou']:.4f}\")\n",
    "            print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"    Recall:    {metrics['recall']:.4f}\")\n",
    "            print(f\"    Frames:    {metrics['num_frames']}\")\n",
    "            \n",
    "            results.append(metrics)\n",
    "            \n",
    "            # Clear model from memory\n",
    "            del model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {model_name}: {str(e)}\")\n",
    "            results.append({\n",
    "                'model_name': model_name,\n",
    "                'model_file': model_file,\n",
    "                'f1': np.nan,\n",
    "                'iou': np.nan,\n",
    "                'precision': np.nan,\n",
    "                'recall': np.nan,\n",
    "                'num_frames': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # Create DataFrame with results\n",
    "    df_results = pd.DataFrame(results)\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = ['model_name', 'f1', 'iou', 'precision', 'recall', 'num_frames', 'model_file']\n",
    "    if 'error' in df_results.columns:\n",
    "        column_order.append('error')\n",
    "    df_results = df_results[column_order]\n",
    "    \n",
    "    # Sort by F1 score (descending)\n",
    "    df_results = df_results.sort_values('f1', ascending=False)\n",
    "    \n",
    "    # Save results if requested\n",
    "    if save_results:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = f\"lstm_resnet_results_{timestamp}.csv\"\n",
    "        df_results.to_csv(output_file, index=False)\n",
    "\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY OF ALL MODELS\")\n",
    "    print(\"=\"*60)\n",
    "    print(df_results.to_string(index=False))\n",
    "    \n",
    "    # Print best performing model\n",
    "    if not df_results.empty and not df_results['f1'].isna().all():\n",
    "        best_model = df_results.iloc[0]\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"  F1 Score:  {best_model['f1']:.4f}\")\n",
    "        print(f\"  mIoU:      {best_model['iou']:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "# Configuration\n",
    "MODELS_DIR = \"models\"  # Directory containing all your trained models\n",
    "IMAGE_DIR = \"../LSTM-Unet/data_v3_processed/test\"\n",
    "TIME_STEPS = 15\n",
    "KERNEL_SIZE = 5\n",
    "PATCH_SIZE = 256\n",
    "\n",
    "# Run inference on all models\n",
    "results_df = pipeline_all_models(\n",
    "    models_dir=MODELS_DIR,\n",
    "    image_dir=IMAGE_DIR,\n",
    "    time_steps=TIME_STEPS,\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    model_extensions=['.h5'],\n",
    "    save_results=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
